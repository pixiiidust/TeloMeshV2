# ðŸ“ telomesh/ â€” MVP SLC Implementation (Simple + Lovable)

# Cursor Rule: Directory Hygiene Protocol
# ----------------------------------------
# Every file must:
# 1. Contain a clearly scoped set of related functions.
# 2. Be directly tied to a specific output or UI view.
# 3. Be covered by at least 1 test in /tests matching its prefix.
# 4. Follow the directory below â€” DO NOT create new folders without approval.
# 5. New files must register here and declare tests â€” else trigger the hygiene fix loop.

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ðŸ“ data/                     # SIMPLE STAGE â€” Events Generation
â”œâ”€â”€ synthetic_event_generator.py     # Generate mock events
â”‚   â””â”€â”€ generate_synthetic_events(n_users: int, events_per_user: int) -> pd.DataFrame
â”œâ”€â”€ synthetic_data.md        # Documentation about data generation
â”œâ”€â”€ [dataset_name]/          # Dataset-specific input data
â”‚   â””â”€â”€ events.csv           # Raw event data for the dataset

ðŸ“ ingest/                     # SIMPLE STAGE â€” Event â†’ Flow â†’ Graph
â”œâ”€â”€ parse_sessions.py         # Converts CSV â†’ structured flows
â”‚   â””â”€â”€ parse_sessions(input_csv: str, output_csv: str) -> pd.DataFrame
â”‚
â”œâ”€â”€ build_graph.py            # Builds session path graph
â”‚   â””â”€â”€ build_graph(input_csv: str, output_graph: str, multi_graph: bool) -> nx.DiGraph or nx.MultiDiGraph
â”‚
â”œâ”€â”€ flow_metrics.py           # Validates sessions and graph
â”‚   â””â”€â”€ validate_sessions(session_flows_csv: str) -> dict
â”‚   â””â”€â”€ validate_graph(graph_pickle: str) -> dict
â”‚   â””â”€â”€ run_metrics() -> None
â”‚
â”œâ”€â”€ data_pipeline.md          # Documentation about data ingestion process

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ðŸ“ analysis/                   # LOVABLE STAGE â€” Metrics & Friction
â”œâ”€â”€ event_chokepoints.py      # Compute friction metrics
â”‚   â””â”€â”€ compute_exit_rates(df: pd.DataFrame) -> pd.DataFrame
â”‚   â””â”€â”€ compute_betweenness(g: nx.DiGraph) -> Dict[str, float]
â”‚   â””â”€â”€ compute_wsjf_friction(exit_df, centrality_dict) -> pd.DataFrame
â”‚   â””â”€â”€ detect_fragile_flows(session_df: pd.DataFrame) -> pd.DataFrame
â”‚   â””â”€â”€ export_chokepoints(df, flow_df, node_map) -> None
â”‚   â””â”€â”€ convert_to_digraph(G: nx.MultiDiGraph) -> nx.DiGraph
â”‚   â””â”€â”€ compute_fractal_dimension(G: nx.DiGraph, max_radius: int = 10) -> float
â”‚   â””â”€â”€ compute_power_law_alpha(G: nx.DiGraph) -> float
â”‚   â””â”€â”€ detect_repeating_subgraphs(G: nx.DiGraph, max_length: int = 4) -> List[List[str]]
â”‚   â””â”€â”€ simulate_percolation(G: nx.DiGraph, ranked_nodes: List = None, threshold: float = 0.5, fast: bool = False) -> float
â”‚   â””â”€â”€ compute_fractal_betweenness(G: nx.DiGraph, repeating_subgraphs: List = None, centrality: Dict = None) -> Dict[str, float]
â”‚   â””â”€â”€ build_decision_table(G: nx.DiGraph, D: float, alpha: float, FB: Dict, threshold: float, chokepoints: pd.DataFrame, cc: float = None) -> pd.DataFrame
â”‚   â””â”€â”€ compute_clustering_coefficient(G: nx.DiGraph) -> float
â”‚   â””â”€â”€ main(input_flows: str, input_graph: str, input_graph_multi: str, output_dir: str, fast: bool = False) -> Tuple[pd.DataFrame, pd.DataFrame, Dict]
â”‚
â”œâ”€â”€ friction_analysis.md      # Documentation about friction analysis

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ðŸ“ ui/                         # LOVABLE STAGE â€” PM-Facing Dashboard
â”œâ”€â”€ dashboard.py              # Streamlit dashboard
â”‚   â””â”€â”€ load_friction_data() -> Tuple[DataFrame, DataFrame, Dict]
â”‚   â””â”€â”€ render_friction_table(df: pd.DataFrame)
â”‚   â””â”€â”€ render_network_graph(net, height=1000) -> str
â”‚   â””â”€â”€ render_graph_heatmap(graph: nx.DiGraph, score_map: Dict[str, float])
â”‚   â””â”€â”€ render_flow_summaries(flow_df: pd.DataFrame)
â”‚   â””â”€â”€ render_transition_pairs(flow_df: pd.DataFrame)
â”‚   â””â”€â”€ load_advanced_metrics(dataset: str = None) -> Dict
â”‚   â””â”€â”€ render_fb_vs_wsjf_chart(metrics_data: Dict)
â”‚   â””â”€â”€ render_recurring_patterns(metrics_data: Dict)
â”‚   â””â”€â”€ render_percolation_collapse(metrics_data: Dict)
â”‚   â””â”€â”€ render_glossary_sidebar()
â”‚   â””â”€â”€ render_advanced_metrics_tab(metrics_data: Dict)
â”‚   â””â”€â”€ discover_datasets() -> List[str]
â”‚   â””â”€â”€ load_dataset_metadata(dataset_name: str) -> Dict
â”‚   â””â”€â”€ create_full_network(graph, score_map, top10_threshold, top20_threshold, physics_enabled, top50_threshold=None, layout_type="friction_levels") -> Network
â”‚   â””â”€â”€ create_filtered_network(graph, filtered_nodes, top10_threshold, top20_threshold, physics_enabled, top50_threshold=None, layout_type="friction_levels") -> Network
â”‚
â”œâ”€â”€ dashboard_backup.py       # Backup of the latest dashboard version
â”‚
â”œâ”€â”€ dashboard_components.md   # Documentation about dashboard components

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ðŸ“ utils/                      # Utility Tools
â”œâ”€â”€ analytics_converter.py    # Convert from various analytics platforms
â”‚   â””â”€â”€ convert_data(input_file: str, output_file: str, format: str, telomesh_format: bool) -> pd.DataFrame
â”‚   â””â”€â”€ generate_sample_data(format: str, output_file: str) -> pd.DataFrame
â”‚   â””â”€â”€ map_to_telomesh_format(df: pd.DataFrame, format: str) -> pd.DataFrame
â”œâ”€â”€ README.md                 # Comprehensive documentation for analytics converter

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ðŸ“ outputs/                    # Data generated by pipeline
â”œâ”€â”€ [dataset_name]/           # Dataset-specific outputs
â”‚   â”œâ”€â”€ session_flows.csv         # From parse_sessions.py
â”‚   â”œâ”€â”€ user_graph.gpickle        # From build_graph.py
â”‚   â”œâ”€â”€ user_graph_multi.gpickle  # From build_graph.py (for advanced analysis)
â”‚   â”œâ”€â”€ event_chokepoints.csv     # From event_chokepoints.py
â”‚   â”œâ”€â”€ high_friction_flows.csv   # From event_chokepoints.py
â”‚   â”œâ”€â”€ friction_node_map.json    # From event_chokepoints.py
â”‚   â”œâ”€â”€ decision_table.csv        # From event_chokepoints.py (UX recommendations)
â”‚   â”œâ”€â”€ metrics.json              # From event_chokepoints.py (network metrics)
â”‚   â”œâ”€â”€ dataset_info.json         # Dataset metadata
â”‚   â””â”€â”€ session_stats.log         # From flow_metrics.py
â”œâ”€â”€ analysis_results.md           # Documentation about output files

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ðŸ“ logs/                      # Validation logs (legacy)
â”œâ”€â”€ session_stats.log            # From flow_metrics.py (legacy)
â”œâ”€â”€ metrics.json                 # From flow_metrics.py (legacy)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ðŸ“ tests/                     # Enforces Cursor TDD loop
â”œâ”€â”€ test_synthetic_events.py
â”œâ”€â”€ test_parse_sessions.py
â”œâ”€â”€ test_build_graph.py
â”œâ”€â”€ test_flow_metrics.py
â”œâ”€â”€ test_event_chokepoints.py    # Tests for all event_chokepoints.py functions
â”œâ”€â”€ test_dashboard_ui.py
â”œâ”€â”€ test_advanced_metrics_tab.py # Tests for advanced metrics visualizations
â”œâ”€â”€ test_analytics_converter.py
â”œâ”€â”€ test_dataset_organization.py  # Tests for dataset organization

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ðŸ“ logos/                     # Visual assets
â”œâ”€â”€ telomesh logo.png           # Main logo
â””â”€â”€ telomesh logo white.png     # White version for dark backgrounds

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

main.py                       # Pipeline entrypoint
â””â”€â”€ run_all_stages(dataset_name: str, users: int, events_per_user: int, input_file: str, multi: bool = False, fast: bool = False) -> None
â””â”€â”€ create_dataset_directories(dataset_name: str) -> Tuple[str, str]
â””â”€â”€ generate_dataset_metadata(dataset_name: str, data_dir: str, output_dir: str, users_count: int, events_count: int, sessions_count: int) -> Dict

