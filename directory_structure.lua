# ðŸ“ telomesh/ â€” MVP SLC Implementation (Simple + Lovable)

# Cursor Rule: Directory Hygiene Protocol
# ----------------------------------------
# Every file must:
# 1. Contain a clearly scoped set of related functions.
# 2. Be directly tied to a specific output or UI view.
# 3. Be covered by at least 1 test in /tests matching its prefix.
# 4. Follow the directory below â€” DO NOT create new folders without approval.
# 5. New files must register here and declare tests â€” else trigger the hygiene fix loop.

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ðŸ“ data/                     # SIMPLE STAGE â€” Events Generation
â”œâ”€â”€ synthetic_event_generator.py     # Generate mock events
â”‚   â””â”€â”€ generate_synthetic_events(n_users: int, events_per_user: int) -> pd.DataFrame
â”œâ”€â”€ synthetic_data.md        # Documentation about data generation
â”œâ”€â”€ [dataset_name]/          # Dataset-specific input data
â”‚   â””â”€â”€ events.csv           # Raw event data for the dataset

ðŸ“ ingest/                     # SIMPLE STAGE â€” Event â†’ Flow â†’ Graph
â”œâ”€â”€ parse_sessions.py         # Converts CSV â†’ structured flows
â”‚   â””â”€â”€ parse_sessions(input_csv: str, output_csv: str) -> pd.DataFrame
â”‚
â”œâ”€â”€ build_graph.py            # Builds session path graph
â”‚   â””â”€â”€ build_graph(input_csv: str, output_graph: str) -> nx.DiGraph
â”‚
â”œâ”€â”€ flow_metrics.py           # Validates sessions and graph
â”‚   â””â”€â”€ validate_sessions(session_flows_csv: str) -> dict
â”‚   â””â”€â”€ validate_graph(graph_pickle: str) -> dict
â”‚   â””â”€â”€ run_metrics() -> None
â”‚
â”œâ”€â”€ data_pipeline.md          # Documentation about data ingestion process

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ðŸ“ analysis/                   # LOVABLE STAGE â€” Metrics & Friction
â”œâ”€â”€ event_chokepoints.py      # Compute friction metrics
â”‚   â””â”€â”€ compute_exit_rates(df: pd.DataFrame) -> pd.DataFrame
â”‚   â””â”€â”€ compute_betweenness(g: nx.DiGraph) -> Dict[str, float]
â”‚   â””â”€â”€ compute_wsjf_friction(exit_df, centrality_dict) -> pd.DataFrame
â”‚   â””â”€â”€ detect_fragile_flows(session_df: pd.DataFrame) -> pd.DataFrame
â”‚   â””â”€â”€ export_chokepoints(df, flow_df, node_map) -> None
â”‚
â”œâ”€â”€ friction_analysis.md      # Documentation about friction analysis

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ðŸ“ ui/                         # LOVABLE STAGE â€” PM-Facing Dashboard
â”œâ”€â”€ dashboard.py              # Streamlit dashboard
â”‚   â””â”€â”€ load_friction_data() -> Tuple[DataFrame, DataFrame, Dict]
â”‚   â””â”€â”€ render_friction_table(df: pd.DataFrame)
â”‚   â””â”€â”€ render_graph_heatmap(graph: nx.DiGraph, score_map: Dict[str, float])
â”‚   â””â”€â”€ render_flow_summaries(flow_df: pd.DataFrame)
â”‚   â””â”€â”€ render_tooltips(metric: str) -> str
â”‚   â””â”€â”€ discover_datasets() -> List[str]
â”‚   â””â”€â”€ load_dataset_metadata(dataset_name: str) -> Dict
â”‚
â”œâ”€â”€ dashboard_components.md   # Documentation about dashboard components

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ðŸ“ utils/                      # Utility Tools
â”œâ”€â”€ analytics_converter.py    # Convert from various analytics platforms
â”‚   â””â”€â”€ convert_data(input_file: str, output_file: str, format: str, telomesh_format: bool) -> pd.DataFrame
â”‚   â””â”€â”€ generate_sample_data(format: str, output_file: str) -> pd.DataFrame
â”‚   â””â”€â”€ map_to_telomesh_format(df: pd.DataFrame, format: str) -> pd.DataFrame
â”œâ”€â”€ README.md                 # Brief overview of utilities
â”œâ”€â”€ GUIDE.md                  # Detailed converter documentation

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ðŸ“ outputs/                    # Data generated by pipeline
â”œâ”€â”€ [dataset_name]/           # Dataset-specific outputs
â”‚   â”œâ”€â”€ session_flows.csv         # From parse_sessions.py
â”‚   â”œâ”€â”€ user_graph.gpickle        # From build_graph.py
â”‚   â”œâ”€â”€ event_chokepoints.csv     # From event_chokepoints.py
â”‚   â”œâ”€â”€ high_friction_flows.csv   # From event_chokepoints.py
â”‚   â”œâ”€â”€ friction_node_map.json    # From event_chokepoints.py
â”‚   â”œâ”€â”€ dataset_info.json         # Dataset metadata
â”‚   â”œâ”€â”€ metrics.json              # From flow_metrics.py
â”‚   â””â”€â”€ session_stats.log         # From flow_metrics.py

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ðŸ“ logs/                      # Validation logs (legacy)
â”œâ”€â”€ session_stats.log            # From flow_metrics.py (legacy)
â”œâ”€â”€ metrics.json                 # From flow_metrics.py (legacy)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ðŸ“ tests/                     # Enforces Cursor TDD loop
â”œâ”€â”€ test_synthetic_events.py
â”œâ”€â”€ test_parse_sessions.py
â”œâ”€â”€ test_build_graph.py
â”œâ”€â”€ test_flow_metrics.py
â”œâ”€â”€ test_event_chokepoints.py
â”œâ”€â”€ test_dashboard_ui.py
â”œâ”€â”€ test_analytics_converter.py
â”œâ”€â”€ test_dataset_organization.py  # Tests for dataset organization

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ðŸ“ logos/                     # Visual assets
â”œâ”€â”€ telomesh logo.png           # Main logo
â””â”€â”€ telomesh logo white.png     # White version for dark backgrounds

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

main.py                       # Pipeline entrypoint
â””â”€â”€ run_all_stages(dataset_name: str, users: int, events_per_user: int, input_file: str) -> None
â””â”€â”€ create_dataset_directories(dataset_name: str) -> Tuple[str, str]
â””â”€â”€ generate_dataset_metadata(dataset_name: str, data_dir: str, output_dir: str, users_count: int, events_count: int, sessions_count: int) -> Dict

